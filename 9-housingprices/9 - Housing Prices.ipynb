{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': \"Boston House Prices dataset\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\",\n",
       " 'data': array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00, ...,\n",
       "           1.53000000e+01,   3.96900000e+02,   4.98000000e+00],\n",
       "        [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
       "           1.78000000e+01,   3.96900000e+02,   9.14000000e+00],\n",
       "        [  2.72900000e-02,   0.00000000e+00,   7.07000000e+00, ...,\n",
       "           1.78000000e+01,   3.92830000e+02,   4.03000000e+00],\n",
       "        ..., \n",
       "        [  6.07600000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "           2.10000000e+01,   3.96900000e+02,   5.64000000e+00],\n",
       "        [  1.09590000e-01,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "           2.10000000e+01,   3.93450000e+02,   6.48000000e+00],\n",
       "        [  4.74100000e-02,   0.00000000e+00,   1.19300000e+01, ...,\n",
       "           2.10000000e+01,   3.96900000e+02,   7.88000000e+00]]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], \n",
       "       dtype='|S7'),\n",
       " 'target': array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "         18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "         20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "         14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "         20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "         19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "         18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "         25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "         22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "         23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "         22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "         33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "         19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "         19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "         15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "         18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "         11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "         19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "         50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "         19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "         39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "         34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "         34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "         22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "         23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "         50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "         29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "         23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "         29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "         43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "         21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "         35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "         22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "         20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "         33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "         16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "         25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "         22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "         16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "         24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "         25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "         50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "         13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "          7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "         12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "          5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "          7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "         13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "         16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "         17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "         13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "         20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "         19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "         12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "         19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "         23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "         22. ,  11.9])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load the Boston dataset.\"\"\"\n",
    "\n",
    "    boston = datasets.load_boston()\n",
    "    return boston\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_city_data(city_data):\n",
    "    \"\"\"Calculate the Boston housing statistics.\"\"\"\n",
    "\n",
    "    # Get the labels and features from the housing data\n",
    "    housing_prices = city_data.target\n",
    "    housing_features = city_data.data\n",
    "\n",
    "    ###################################\n",
    "    ### Step 1. YOUR CODE GOES HERE ###\n",
    "    \n",
    "    print \"Number of houses: \", len(housing_prices)\n",
    "    feature_names = city_data.feature_names\n",
    "    print \"Number of features: \", len(feature_names)\n",
    "    print \"Min Price: \", np.min(housing_prices) \n",
    "    print \"Max Price: \", np.max(housing_prices)\n",
    "    print \"Mean Price: \", np.mean(housing_prices) \n",
    "    print \"Median Price: \", np.median(housing_prices)\n",
    "    print \"Std Price: \", np.std(housing_prices) \n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Please calculate the following values using the Numpy library\n",
    "    # Size of data (number of houses)?\n",
    "    # Number of features?\n",
    "    # Minimum price?\n",
    "    # Maximum price?\n",
    "    # Calculate mean price?\n",
    "    # Calculate median price?\n",
    "    # Calculate standard deviation?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(city_data):\n",
    "    \"\"\"Randomly shuffle the sample set. Divide it into 70 percent training and 30 percent testing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "\n",
    "    ###################################\n",
    "    ### Step 2. YOUR CODE GOES HERE ###\n",
    "    df1 = pd.DataFrame(data = X)\n",
    "    df2 = pd.DataFrame(data = y , columns = [\"price\"])\n",
    "    df3 = pd.concat([df1,df2], axis = 1)\n",
    "    df4 = df3.reindex(np.random.permutation(df3.index))\n",
    "    \n",
    "    training_size = int(len(y)*0.7)\n",
    "    testing_size = len(y)-training_size\n",
    "    \n",
    "    X_train = np.array(df4.iloc[:training_size,:13].values)\n",
    "    y_train = np.array(df4.iloc[:training_size,-1:].values)\n",
    "    \n",
    "    X_test = np.array(df4.iloc[-testing_size:,:13].values)\n",
    "    y_test = np.array(df4.iloc[-testing_size:,-1:].values)\n",
    "    \n",
    "    ###################################\n",
    "    print training_size\n",
    "    print X_train\n",
    "    print y_train\n",
    "    print X_test\n",
    "    print y_test\n",
    "    #return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance_metric(label, prediction):\n",
    "    \"\"\"Calculate and return the appropriate error performance metric.\"\"\"\n",
    "\n",
    "    ###################################\n",
    "    ### Step 3. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "\n",
    "    # The following page has a table of scoring functions in sklearn:\n",
    "    # http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curve(depth, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model after a set of training data.\"\"\"\n",
    "\n",
    "    # We will vary the training set size so that we have 50 different sizes\n",
    "    sizes = np.round(np.linspace(1, len(X_train), 50))\n",
    "    train_err = np.zeros(len(sizes))\n",
    "    test_err = np.zeros(len(sizes))\n",
    "\n",
    "    print \"Decision Tree with Max Depth: \"\n",
    "    print depth\n",
    "\n",
    "    for i, s in enumerate(sizes):\n",
    "\n",
    "        # Create and fit the decision tree regressor model\n",
    "        regressor = DecisionTreeRegressor(max_depth=depth)\n",
    "        regressor.fit(X_train[:s], y_train[:s])\n",
    "\n",
    "        # Find the performance on the training and testing set\n",
    "        train_err[i] = performance_metric(y_train[:s], regressor.predict(X_train[:s]))\n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "\n",
    "    # Plot learning curve graph\n",
    "    learning_curve_graph(sizes, train_err, test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curve_graph(sizes, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the training size.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Training Size')\n",
    "    pl.plot(sizes, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(sizes, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Training Size')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_complexity(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Calculate the performance of the model as model complexity increases.\"\"\"\n",
    "\n",
    "    print \"Model Complexity: \"\n",
    "\n",
    "    # We will vary the depth of decision trees from 2 to 25\n",
    "    max_depth = np.arange(1, 25)\n",
    "    train_err = np.zeros(len(max_depth))\n",
    "    test_err = np.zeros(len(max_depth))\n",
    "\n",
    "    for i, d in enumerate(max_depth):\n",
    "        # Setup a Decision Tree Regressor so that it learns a tree with depth d\n",
    "        regressor = DecisionTreeRegressor(max_depth=d)\n",
    "\n",
    "        # Fit the learner to the training data\n",
    "        regressor.fit(X_train, y_train)\n",
    "\n",
    "        # Find the performance on the training set\n",
    "        train_err[i] = performance_metric(y_train, regressor.predict(X_train))\n",
    "\n",
    "        # Find the performance on the testing set\n",
    "        test_err[i] = performance_metric(y_test, regressor.predict(X_test))\n",
    "\n",
    "    # Plot the model complexity graph\n",
    "    model_complexity_graph(max_depth, train_err, test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_complexity_graph(max_depth, train_err, test_err):\n",
    "    \"\"\"Plot training and test error as a function of the depth of the decision tree learn.\"\"\"\n",
    "\n",
    "    pl.figure()\n",
    "    pl.title('Decision Trees: Performance vs Max Depth')\n",
    "    pl.plot(max_depth, test_err, lw=2, label = 'test error')\n",
    "    pl.plot(max_depth, train_err, lw=2, label = 'training error')\n",
    "    pl.legend()\n",
    "    pl.xlabel('Max Depth')\n",
    "    pl.ylabel('Error')\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_model(city_data):\n",
    "    \"\"\"Find and tune the optimal model. Make a prediction on housing data.\"\"\"\n",
    "\n",
    "    # Get the features and labels from the Boston housing data\n",
    "    X, y = city_data.data, city_data.target\n",
    "\n",
    "    # Setup a Decision Tree Regressor\n",
    "    regressor = DecisionTreeRegressor()\n",
    "\n",
    "    parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}\n",
    "\n",
    "    ###################################\n",
    "    ### Step 4. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "\n",
    "    # 1. Find an appropriate performance metric. This should be the same as the\n",
    "    # one used in your performance_metric procedure above:\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "\n",
    "    # 2. We will use grid search to fine tune the Decision Tree Regressor and\n",
    "    # obtain the parameters that generate the best training performance. Set up\n",
    "    # the grid search object here.\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV\n",
    "\n",
    "    # Fit the learner to the training data to obtain the best parameter set\n",
    "    print \"Final Model: \"\n",
    "    print reg.fit(X, y)\n",
    "    \n",
    "    # Use the model to predict the output of a particular sample\n",
    "    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]\n",
    "    y = reg.predict(x)\n",
    "    print \"House: \" + str(x)\n",
    "    print \"Prediction: \" + str(y)\n",
    "\n",
    "#In the case of the documentation page for GridSearchCV, it might be the case that the example is just a demonstration of syntax for use of the function, rather than a statement about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of houses:  506\n",
      "Number of features:  13\n",
      "Min Price:  5.0\n",
      "Max Price:  50.0\n",
      "Mean Price:  22.5328063241\n",
      "Median Price:  21.2\n",
      "Std Price:  9.18801154528\n",
      "354\n",
      "[[  4.52700000e-02   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.96900000e+02   9.08000000e+00]\n",
      " [  5.37200000e-02   0.00000000e+00   1.39200000e+01 ...,   1.60000000e+01\n",
      "    3.92850000e+02   7.39000000e+00]\n",
      " [  7.50260000e-01   0.00000000e+00   8.14000000e+00 ...,   2.10000000e+01\n",
      "    3.94330000e+02   1.63000000e+01]\n",
      " ..., \n",
      " [  8.49213000e+00   0.00000000e+00   1.81000000e+01 ...,   2.02000000e+01\n",
      "    8.34500000e+01   1.76400000e+01]\n",
      " [  8.24809000e+00   0.00000000e+00   1.81000000e+01 ...,   2.02000000e+01\n",
      "    3.75870000e+02   1.67400000e+01]\n",
      " [  9.88430000e-01   0.00000000e+00   8.14000000e+00 ...,   2.10000000e+01\n",
      "    3.94540000e+02   1.98800000e+01]]\n",
      "[[ 20.6]\n",
      " [ 27.1]\n",
      " [ 15.6]\n",
      " [ 22.3]\n",
      " [ 24.7]\n",
      " [ 14.9]\n",
      " [ 25. ]\n",
      " [ 21. ]\n",
      " [ 20.9]\n",
      " [ 20.8]\n",
      " [ 19.8]\n",
      " [ 22.8]\n",
      " [ 19.9]\n",
      " [ 20.2]\n",
      " [ 13.8]\n",
      " [ 13. ]\n",
      " [ 19.1]\n",
      " [ 23. ]\n",
      " [ 14.4]\n",
      " [ 20. ]\n",
      " [ 18.2]\n",
      " [ 22.2]\n",
      " [ 19.3]\n",
      " [ 20. ]\n",
      " [ 23.9]\n",
      " [ 17.2]\n",
      " [ 32.2]\n",
      " [ 46. ]\n",
      " [ 35.1]\n",
      " [ 23.1]\n",
      " [ 10.4]\n",
      " [ 17.5]\n",
      " [  8.1]\n",
      " [ 25. ]\n",
      " [ 23.8]\n",
      " [ 25. ]\n",
      " [ 35.4]\n",
      " [ 19.6]\n",
      " [ 14.4]\n",
      " [ 12.3]\n",
      " [ 18.7]\n",
      " [  9.6]\n",
      " [ 50. ]\n",
      " [ 20.6]\n",
      " [ 22. ]\n",
      " [ 19.1]\n",
      " [ 24.8]\n",
      " [ 12.8]\n",
      " [ 11.7]\n",
      " [ 36.1]\n",
      " [ 30.8]\n",
      " [ 35.2]\n",
      " [ 34.9]\n",
      " [ 16.7]\n",
      " [ 22.1]\n",
      " [ 25. ]\n",
      " [ 12.7]\n",
      " [ 24.8]\n",
      " [ 15. ]\n",
      " [ 21.5]\n",
      " [ 39.8]\n",
      " [ 32. ]\n",
      " [ 46.7]\n",
      " [ 13.5]\n",
      " [ 20.4]\n",
      " [ 25.2]\n",
      " [ 19.3]\n",
      " [ 13.4]\n",
      " [ 44. ]\n",
      " [ 10.5]\n",
      " [ 16.4]\n",
      " [ 18.3]\n",
      " [ 24.6]\n",
      " [  8.4]\n",
      " [ 27.5]\n",
      " [ 33.1]\n",
      " [ 15.6]\n",
      " [ 18.5]\n",
      " [ 13.8]\n",
      " [ 13.1]\n",
      " [ 19.9]\n",
      " [ 20.3]\n",
      " [ 19.4]\n",
      " [ 22. ]\n",
      " [ 19.8]\n",
      " [ 21.2]\n",
      " [ 50. ]\n",
      " [ 18.5]\n",
      " [ 23.8]\n",
      " [ 42.8]\n",
      " [ 17.5]\n",
      " [ 21.9]\n",
      " [ 27.5]\n",
      " [ 24.1]\n",
      " [ 13.4]\n",
      " [ 24.8]\n",
      " [ 27.9]\n",
      " [ 21.8]\n",
      " [ 31.7]\n",
      " [  7. ]\n",
      " [ 22. ]\n",
      " [ 23. ]\n",
      " [ 50. ]\n",
      " [ 30.7]\n",
      " [ 21. ]\n",
      " [  5.6]\n",
      " [ 19.7]\n",
      " [ 34.7]\n",
      " [ 19.6]\n",
      " [  5. ]\n",
      " [ 17.6]\n",
      " [ 12.6]\n",
      " [ 36. ]\n",
      " [  9.7]\n",
      " [ 21.4]\n",
      " [ 50. ]\n",
      " [ 25.3]\n",
      " [ 21.2]\n",
      " [ 13.5]\n",
      " [ 16.5]\n",
      " [ 20.1]\n",
      " [ 32.9]\n",
      " [ 13.1]\n",
      " [ 19.3]\n",
      " [ 20.9]\n",
      " [ 24.6]\n",
      " [ 22.5]\n",
      " [ 13.8]\n",
      " [ 31.6]\n",
      " [ 15. ]\n",
      " [ 22.8]\n",
      " [ 10.2]\n",
      " [ 36.4]\n",
      " [ 14.1]\n",
      " [ 26.4]\n",
      " [ 30.3]\n",
      " [ 48.5]\n",
      " [ 26.5]\n",
      " [ 28.7]\n",
      " [  7.2]\n",
      " [ 20.7]\n",
      " [  8.7]\n",
      " [ 18.9]\n",
      " [ 23.4]\n",
      " [ 23.3]\n",
      " [ 23.3]\n",
      " [ 15.7]\n",
      " [ 24.8]\n",
      " [ 22.4]\n",
      " [ 16.8]\n",
      " [ 19.3]\n",
      " [ 21.6]\n",
      " [ 25. ]\n",
      " [ 29.1]\n",
      " [ 24.3]\n",
      " [ 22.2]\n",
      " [ 13.3]\n",
      " [ 17.1]\n",
      " [ 18.9]\n",
      " [ 19.4]\n",
      " [ 12.7]\n",
      " [ 19.4]\n",
      " [ 10.4]\n",
      " [ 45.4]\n",
      " [ 23.6]\n",
      " [ 25. ]\n",
      " [ 19.8]\n",
      " [ 19. ]\n",
      " [ 22.6]\n",
      " [ 31.2]\n",
      " [ 21.9]\n",
      " [ 36.5]\n",
      " [ 28.7]\n",
      " [ 23.1]\n",
      " [  7.2]\n",
      " [ 34.6]\n",
      " [ 17.5]\n",
      " [ 43.8]\n",
      " [ 50. ]\n",
      " [ 23.2]\n",
      " [ 32.4]\n",
      " [ 25.1]\n",
      " [ 23.1]\n",
      " [ 11.9]\n",
      " [ 50. ]\n",
      " [ 37. ]\n",
      " [ 23.4]\n",
      " [ 23.2]\n",
      " [ 48.3]\n",
      " [ 21.8]\n",
      " [ 19.9]\n",
      " [ 10.2]\n",
      " [ 23.7]\n",
      " [ 26.2]\n",
      " [ 24.5]\n",
      " [ 21.1]\n",
      " [ 11.8]\n",
      " [ 50. ]\n",
      " [ 19.2]\n",
      " [ 29.6]\n",
      " [ 23.1]\n",
      " [  8.8]\n",
      " [ 22.6]\n",
      " [ 14.6]\n",
      " [ 17.1]\n",
      " [ 50. ]\n",
      " [ 19.4]\n",
      " [ 22.8]\n",
      " [ 20.4]\n",
      " [ 14.3]\n",
      " [ 34.9]\n",
      " [  7.4]\n",
      " [ 17.4]\n",
      " [ 19.5]\n",
      " [ 24. ]\n",
      " [ 13.4]\n",
      " [ 27.1]\n",
      " [ 12.1]\n",
      " [ 50. ]\n",
      " [  8.4]\n",
      " [ 13.3]\n",
      " [  5. ]\n",
      " [ 11.8]\n",
      " [ 30.1]\n",
      " [ 15.3]\n",
      " [ 29.9]\n",
      " [ 32.5]\n",
      " [ 22.7]\n",
      " [ 24.4]\n",
      " [ 18.6]\n",
      " [ 22.3]\n",
      " [ 18.4]\n",
      " [ 16.8]\n",
      " [ 21.4]\n",
      " [ 19. ]\n",
      " [ 24.5]\n",
      " [ 21.4]\n",
      " [ 26.6]\n",
      " [ 50. ]\n",
      " [ 15.6]\n",
      " [ 13.4]\n",
      " [  7.2]\n",
      " [ 23.1]\n",
      " [  8.3]\n",
      " [ 20. ]\n",
      " [ 15.2]\n",
      " [ 20.1]\n",
      " [ 28.4]\n",
      " [ 13.3]\n",
      " [ 15.2]\n",
      " [ 11. ]\n",
      " [ 28. ]\n",
      " [  8.8]\n",
      " [ 16. ]\n",
      " [ 20. ]\n",
      " [ 36.2]\n",
      " [ 24.4]\n",
      " [ 19.6]\n",
      " [ 19.1]\n",
      " [ 18.9]\n",
      " [ 33.2]\n",
      " [ 36.2]\n",
      " [ 28.1]\n",
      " [ 41.3]\n",
      " [ 25. ]\n",
      " [ 30.1]\n",
      " [ 26.4]\n",
      " [ 15.1]\n",
      " [ 19.5]\n",
      " [ 20.1]\n",
      " [ 18.4]\n",
      " [ 16.2]\n",
      " [ 19.7]\n",
      " [ 31.5]\n",
      " [  7. ]\n",
      " [ 18.3]\n",
      " [ 50. ]\n",
      " [ 11.7]\n",
      " [ 42.3]\n",
      " [ 10.5]\n",
      " [ 21.2]\n",
      " [ 13.8]\n",
      " [ 23.2]\n",
      " [ 20.5]\n",
      " [ 17.4]\n",
      " [  6.3]\n",
      " [ 24.2]\n",
      " [ 26.6]\n",
      " [ 17.7]\n",
      " [ 13.6]\n",
      " [ 33.3]\n",
      " [ 26.7]\n",
      " [ 22.2]\n",
      " [ 27.5]\n",
      " [ 15. ]\n",
      " [ 16.6]\n",
      " [  9.5]\n",
      " [ 21.4]\n",
      " [ 23.1]\n",
      " [ 18.9]\n",
      " [ 13.1]\n",
      " [  8.3]\n",
      " [ 12. ]\n",
      " [ 23.7]\n",
      " [ 10.8]\n",
      " [ 23.9]\n",
      " [ 14.5]\n",
      " [ 24.4]\n",
      " [ 21.6]\n",
      " [ 14.1]\n",
      " [ 25. ]\n",
      " [ 10.9]\n",
      " [ 10.9]\n",
      " [ 33.2]\n",
      " [ 16.1]\n",
      " [ 29. ]\n",
      " [ 15.4]\n",
      " [ 24.1]\n",
      " [ 18.5]\n",
      " [ 20.2]\n",
      " [ 15.6]\n",
      " [ 18.2]\n",
      " [ 22.9]\n",
      " [ 16.5]\n",
      " [ 21.7]\n",
      " [ 18.8]\n",
      " [ 16.1]\n",
      " [ 50. ]\n",
      " [ 24.3]\n",
      " [ 27.5]\n",
      " [ 22. ]\n",
      " [ 22. ]\n",
      " [ 15.2]\n",
      " [ 16.7]\n",
      " [ 19.4]\n",
      " [ 18. ]\n",
      " [ 21. ]\n",
      " [ 18.8]\n",
      " [ 32. ]\n",
      " [ 10.2]\n",
      " [ 20.4]\n",
      " [ 11.5]\n",
      " [ 33.4]\n",
      " [ 23.2]\n",
      " [ 14. ]\n",
      " [ 24. ]\n",
      " [ 23. ]\n",
      " [ 33.8]\n",
      " [ 22.7]\n",
      " [ 21.7]\n",
      " [ 43.5]\n",
      " [ 14.5]\n",
      " [ 17.8]\n",
      " [ 14.5]]\n",
      "[[  5.82401000e+00   0.00000000e+00   1.81000000e+01 ...,   2.02000000e+01\n",
      "    3.96900000e+02   1.07400000e+01]\n",
      " [  3.55100000e-02   2.50000000e+01   4.86000000e+00 ...,   1.90000000e+01\n",
      "    3.90640000e+02   7.51000000e+00]\n",
      " [  5.64600000e-02   0.00000000e+00   1.28300000e+01 ...,   1.87000000e+01\n",
      "    3.86400000e+02   1.23400000e+01]\n",
      " ..., \n",
      " [  2.54300000e-02   5.50000000e+01   3.78000000e+00 ...,   1.76000000e+01\n",
      "    3.96900000e+02   7.18000000e+00]\n",
      " [  4.83567000e+00   0.00000000e+00   1.81000000e+01 ...,   2.02000000e+01\n",
      "    3.88220000e+02   1.14500000e+01]\n",
      " [  1.02900000e-01   3.00000000e+01   4.93000000e+00 ...,   1.66000000e+01\n",
      "    3.72750000e+02   1.12200000e+01]]\n",
      "[[ 23. ]\n",
      " [ 22.9]\n",
      " [ 21.2]\n",
      " [ 31.6]\n",
      " [ 37.9]\n",
      " [ 20.7]\n",
      " [ 29.8]\n",
      " [ 18.4]\n",
      " [ 24.1]\n",
      " [ 23.7]\n",
      " [ 21.7]\n",
      " [ 15.6]\n",
      " [ 14.9]\n",
      " [ 13.2]\n",
      " [ 26.6]\n",
      " [ 14.8]\n",
      " [ 31.1]\n",
      " [ 17. ]\n",
      " [ 23.7]\n",
      " [ 18.2]\n",
      " [ 22.9]\n",
      " [ 14.3]\n",
      " [ 22.5]\n",
      " [ 37.2]\n",
      " [ 17.1]\n",
      " [ 13.9]\n",
      " [ 24.4]\n",
      " [ 21.7]\n",
      " [ 20.5]\n",
      " [ 21.1]\n",
      " [ 17.9]\n",
      " [ 50. ]\n",
      " [ 13.1]\n",
      " [ 30.1]\n",
      " [ 24.3]\n",
      " [ 20.3]\n",
      " [ 21.7]\n",
      " [ 37.3]\n",
      " [ 32.7]\n",
      " [ 17.2]\n",
      " [ 50. ]\n",
      " [ 24.5]\n",
      " [ 48.8]\n",
      " [ 19.6]\n",
      " [ 18.5]\n",
      " [ 16.6]\n",
      " [ 20.8]\n",
      " [ 18.1]\n",
      " [ 21.2]\n",
      " [ 20. ]\n",
      " [ 29.6]\n",
      " [ 12.7]\n",
      " [ 22. ]\n",
      " [ 13.9]\n",
      " [ 18.7]\n",
      " [ 20.5]\n",
      " [ 24.7]\n",
      " [ 12.5]\n",
      " [ 18.6]\n",
      " [ 23.9]\n",
      " [ 13.8]\n",
      " [ 16.1]\n",
      " [ 23.3]\n",
      " [ 23.8]\n",
      " [ 20.8]\n",
      " [ 14.9]\n",
      " [ 19.3]\n",
      " [ 22.6]\n",
      " [ 17.8]\n",
      " [ 30.5]\n",
      " [ 23.9]\n",
      " [ 19.5]\n",
      " [ 31. ]\n",
      " [ 21.7]\n",
      " [ 19.4]\n",
      " [ 11.9]\n",
      " [ 23.5]\n",
      " [ 22.2]\n",
      " [ 21.4]\n",
      " [  7.5]\n",
      " [ 29.8]\n",
      " [ 50. ]\n",
      " [ 14.1]\n",
      " [ 19.2]\n",
      " [ 20.6]\n",
      " [ 17.8]\n",
      " [ 13.6]\n",
      " [ 14.2]\n",
      " [ 22.6]\n",
      " [ 29.1]\n",
      " [ 16.2]\n",
      " [ 37.6]\n",
      " [ 38.7]\n",
      " [ 17.3]\n",
      " [ 22. ]\n",
      " [ 23.6]\n",
      " [ 34.9]\n",
      " [ 31.5]\n",
      " [ 22.6]\n",
      " [ 17.2]\n",
      " [ 24.7]\n",
      " [ 29.4]\n",
      " [ 27.9]\n",
      " [ 20.3]\n",
      " [ 21.9]\n",
      " [ 22.4]\n",
      " [ 43.1]\n",
      " [ 17.8]\n",
      " [ 33.1]\n",
      " [ 33. ]\n",
      " [ 28.7]\n",
      " [ 33.4]\n",
      " [ 28.6]\n",
      " [ 22.8]\n",
      " [ 19.9]\n",
      " [ 20.3]\n",
      " [ 28.5]\n",
      " [ 11.3]\n",
      " [ 20.6]\n",
      " [ 15.4]\n",
      " [ 50. ]\n",
      " [ 22.9]\n",
      " [ 23.8]\n",
      " [ 20.1]\n",
      " [ 23.3]\n",
      " [ 19.5]\n",
      " [ 23.1]\n",
      " [ 28.4]\n",
      " [ 21.5]\n",
      " [ 20.4]\n",
      " [ 35.4]\n",
      " [ 44.8]\n",
      " [ 27. ]\n",
      " [  8.5]\n",
      " [ 29. ]\n",
      " [ 21.7]\n",
      " [ 41.7]\n",
      " [ 19.1]\n",
      " [ 16.3]\n",
      " [ 17.8]\n",
      " [  8.5]\n",
      " [ 28.2]\n",
      " [ 14.6]\n",
      " [ 20.6]\n",
      " [ 17.4]\n",
      " [ 18.7]\n",
      " [ 22.5]\n",
      " [ 19.6]\n",
      " [ 20.1]\n",
      " [ 23.9]\n",
      " [ 20.6]\n",
      " [ 22.2]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-7e988ab48a50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-75-7e988ab48a50>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Training/Test dataset split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Learning Curve Graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Analyze the Boston housing data. Evaluate and validate the\n",
    "    performanance of a Decision Tree regressor on the housing data.\n",
    "    Fine tune the model to make prediction on unseen data.\"\"\"\n",
    "\n",
    "    # Load data\n",
    "    city_data = load_data()\n",
    "\n",
    "    # Explore the data\n",
    "    explore_city_data(city_data)\n",
    "\n",
    "    # Training/Test dataset split\n",
    "    X_train, y_train, X_test, y_test = split_data(city_data)\n",
    "\n",
    "    # Learning Curve Graphs\n",
    "    max_depths = [1,2,3,4,5,6,7,8,9,10]\n",
    "    for max_depth in max_depths:\n",
    "        learning_curve(max_depth, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Model Complexity Graph\n",
    "    model_complexity(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Tune and predict Model\n",
    "    fit_predict_model(city_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
